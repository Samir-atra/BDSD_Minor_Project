- to create the second dataset that contain the happy sad and neutral emotions.
- loaded the fer2013 dataset, take only the happy, sad and neutral emotions 3, 4 and 6.
- split into training, validation and test sets.
- run it through mediapipe to get only valid and understood images.
- find the relevant blendshapes for happy, neutral and sad emotions, sum to 33 blendshapes.
- augment the training set, takeout the ununderstandable images and add the augmented ones. end up with around 30000 images augmented and non-augmented

- create a blendshapes dataset.
- chenged the happy, sad, and neutral labels to 0,1,2, and transfered them to categorical (one-hot) instead of numbers.
- train the model, and compile with categorical crossentropy and adam optimizer with metrics, accuracy and f1-score 
- implement keras tuner for the learning rate and check for improvement.

experiments record:

- training with learning rate = 0.00005 for 100 epochs nothing normalized gives: 
    loss: 0.6518 - accuracy: 0.7014 - f1_score: 0.6602 - val_loss: 0.6610 - val_accuracy: 0.6920 - val_f1_score: 0.6572
- training with learning rate = 0.00005 for 100 epochs train and val normalized gives:
    loss: 0.6358 - accuracy: 0.7094 - f1_score: 0.6693 - val_loss: 0.6490 - val_accuracy: 0.6956 - val_f1_score: 0.6555
    in keras tuner 0.001 is best so far when accuracy is not included.
    when include accuracy and train on the above learning rate, the gradient explodes after around 40 epochs.
    use weight initialization and gradient clipping to tackle that.
- using the gradient clipping, a learning rate = 0.0046 and after the 100 epoch:
    loss: 0.5600 - categorical_accuracy: 0.7457 - f1_score: 0.7129 - val_loss: 0.6512 - val_categorical_accuracy: 0.6966 - val_f1_score: 0.6490
    in this case after running for more than 100 epochs, will give a few jumps in the loss that means the training is failing but 
    accuracy and f1-score stay steadily improving.
- try to improve the fit and reduce bias by adding another lstm layer, keeping the LR and epochs same:
    loss: 0.5220 - categorical_accuracy: 0.7618 - f1_score: 0.7307 - val_loss: 0.6777 - val_categorical_accuracy: 0.6935 - val_f1_score: 0.6604
    after evaluating and testing this one, it does not work because the result will be normalized. so will retrain with out normalization.
- removed normalization and training with same LR and architecture as above to get the following results:
    loss: 0.5789 - categorical_accuracy: 0.7375 - f1_score: 0.7030 - val_loss: 0.6428 - val_categorical_accuracy: 0.7141 - val_f1_score: 0.6738 
    when implemt the model to the gaze project it gives solid results.
    the full model setup is: 
    -   model = tf.keras.Sequential([
        tf.keras.layers.LSTM(units = 33, activation = 'relu', return_sequences= True),
        tf.keras.layers.LSTM(units = 12, activation = 'relu', return_sequences= False),
        tf.keras.layers.Dense(units = 3, activation = 'softmax'),
        ])


        # Compiling the RNN
        model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),
                    optimizer= tf.keras.optimizers.Adam(learning_rate = 0.00464489028556945, clipnorm=1),
                    metrics = [tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.F1Score()])


        # Fitting the RNN to the Training set
        model.fit(x=X_train, y=Y_train, validation_data = (X_val,y_val) ,epochs = 100)



=================================================================================================================================================

to further improve the model: 
try regularization for overfitting then test with different optimizers,  
get the confusion matrix 
get spontanious dataset 


running keras tuner: 
best so far: 
Hyperparameters: 
lr: 0.00020047766795977846 
unites: 8 
beta: 0.7 
Score: 0.7140657305717468 

and: 
Best val_categorical_accuracy So Far: 0.7125256657600403 
|Best Value So Far |Hyperparameter 
|0.0010147         |lr 
|18                |unites 
|0.9               |beta 


- after a few trials without improvement, checked the counts of the classes of the dataset, and equalized them to 7500 each after around 13000, 7500, and 9000, and added a dataset shuffling before training. 
after fixing the dataset it shows significant improvements but it has overfitting so will use dropout and kernel initialization with uniform_glorot to avoid that. 

- after increasing the number of neurons for the hidden layers and maybe they look too much in comparison to the number of the features but experiments give good results and the number of hidden layers themselves, found that with tanh as activation function the model improve the training result but overfit while with relu, it does not get to 70% 

- after a few experiments with different model architectures from making the lstm bidirectional to adding kernel regularizer and recurrent dropout only variance improved but the bias stayed the same, turned to the optimizer again, and noticed the ema_momentum has a big effect on the accuracy so continued and included amsgrad, and run for 300 epochs  

 - after so many trials with lstm and fully dense model think that the quality of the data is affecting the results and better the 73% accuracy is not possible. 

so found the confusion matrix for the dense model to be: 

[[746, 11, 76], 
[ 53, 279, 163], 
[ 51, 293, 242]] 

and for the LSTM test 73% from past week is: 

[[754, 22, 57], 
[ 58, 209, 228], 
[ 54,  91, 441]] 

both cases shows that the major issue happens between the sad and neutral images, and from previous experience the neutral face would be predicted as sad rather than happy 
when the dataset does not include neutral class, so it is either more work on the model or the final product is not so reasonable. 
so lstm is better, and that is encouraging for further experimentation, so will keep the idea of changing the dataset aside for now and work on lstm. 
- now keeping the learning rate in the range e-4 to e-5 and increasing the size of the model to increase its capacity since it stops improving after some epochs with big bias. 
By increasing the number of neurons and hidden layers and using activation function selu because the relu makes the loss explode after a few epochs probably 
because does not have a negative part while tanh works fast but stops improving early, reasoned because the slope goes to zero for big values so from the photo 

 
 and that showed improvement but took much time to get results since the epoch takes more time and more epohs are required. 

 
There will not be a test model this week. 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

- after training for 600 epochs reached 71 validation accuracy again with a steady and smooth improvement line for loss, accuracy and f1 score. 
so now will train for 1000 epochs and then decide what's next.

- started creating the full dataset that includes the happy, sad and unknown emotions, truncate the floating points of the dataset for 4 digits only
then found that the sad class will disappear if thi is the case.

- still training on the same model. while changing the Hyperparameters, and some ideas are assigning weights to the classes or initializing the kernels with a certain range.
- also visualizing the features might give an idea about what is happening, there are 5 non-relevant features.

- the main issue now is that the unknown class is taking over the happy and sad classes. 